path_normalize_factor :  ../temp/normalize_factors.p
/home/wisrl/anaconda3/envs/imu_38/lib/python3.8/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  return torch.load(io.BytesIO(b))
path_normalize_factor :  ../temp/normalize_factors.p
dataset_train_filter_keys :  odict_keys(['merged_output_py_bias_2'])
dataset_list_rpe_keys :  dict_keys(['merged_output_py_bias_2', 'merged_output_py_bias_3'])
The IEKF nets are saved in the file ../temp/iekfnets.p
start time :  1727989937.1029494
/home/wisrl/anaconda3/envs/imu_38/lib/python3.8/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  return torch.load(io.BytesIO(b))
N :  1000 N0 :  48970
clip length:  tensor(38) 2796
precompute lost 1960 1997
min seq : 2, max seq : 11
loss 0 : 1.3890905693956983
0 loss: 1.38909[0m
gradient norm: 2.11647[0m
Train Epoch:  1 	Loss: 1.38909
The IEKF nets are saved in the file ../temp/iekfnets.p
Amount of time spent for 1 epoch: 7s

N :  1000 N0 :  8310
clip length:  tensor(36) 2796
precompute lost 336 371
min seq : 2, max seq : 7
loss 0 : 1.3867383330000103
0 loss: 1.38674[0m
[33mgradient norm: 3.99828[0m
Train Epoch:  2 	Loss: 1.38674
The IEKF nets are saved in the file ../temp/iekfnets.p
Amount of time spent for 2 epoch: 5s

N :  1000 N0 :  57160
clip length:  tensor(39) 2796
precompute lost 2288 2326
min seq : 2, max seq : 14
loss 0 : 0.5834255401432072
0 loss: 0.58343[0m
[33mgradient norm: 3.30888[0m
Train Epoch:  3 	Loss: 0.58343
The IEKF nets are saved in the file ../temp/iekfnets.p
Amount of time spent for 3 epoch: 5s

N :  1000 N0 :  1040
clip length:  tensor(37) 2796
precompute lost 44 80
min seq : 2, max seq : 8
loss 0 : 7.027580797510937
0 loss: 7.02758[0m
[33mgradient norm: 68.31132[0m
Train Epoch:  4 	Loss: 7.02758
The IEKF nets are saved in the file ../temp/iekfnets.p
Amount of time spent for 4 epoch: 5s

N :  1000 N0 :  40130
clip length:  tensor(37) 2796
precompute lost 1608 1644
min seq : 2, max seq : 7
loss 0 : 1.5894342016130731
0 loss: 1.58943[0m
gradient norm: 2.59524[0m
Train Epoch:  5 	Loss: 1.58943
The IEKF nets are saved in the file ../temp/iekfnets.p
Amount of time spent for 5 epoch: 5s

N :  1000 N0 :  14490
clip length:  tensor(40) 2796
precompute lost 580 619
min seq : 2, max seq : 8
loss 0 : 0.8749433665129186
0 loss: 0.87494[0m
gradient norm: 1.15655[0m
Train Epoch:  6 	Loss: 0.87494
The IEKF nets are saved in the file ../temp/iekfnets.p
Amount of time spent for 6 epoch: 5s

N :  1000 N0 :  53080
clip length:  tensor(40) 2796
precompute lost 2124 2163
min seq : 2, max seq : 7
loss 0 : 0.08976536778603747
0 loss: 0.08977[0m
gradient norm: 0.44787[0m
Train Epoch:  7 	Loss: 0.08977
The IEKF nets are saved in the file ../temp/iekfnets.p
Amount of time spent for 7 epoch: 5s

N :  1000 N0 :  3690
clip length:  tensor(40) 2796
precompute lost 148 187
min seq : 2, max seq : 8
loss 0 : 13.343228624937135
0 loss: 13.34323[0m
[33mgradient norm: 25.16644[0m
Train Epoch:  8 	Loss: 13.34323
The IEKF nets are saved in the file ../temp/iekfnets.p
Amount of time spent for 8 epoch: 5s

N :  1000 N0 :  38360
clip length:  tensor(39) 2796
precompute lost 1536 1574
min seq : 2, max seq : 7
loss 0 : 0.04911453257632344
0 loss: 0.04911[0m
gradient norm: 0.45075[0m
Train Epoch:  9 	Loss: 0.04911
The IEKF nets are saved in the file ../temp/iekfnets.p
Amount of time spent for 9 epoch: 5s

N :  1000 N0 :  38970
clip length:  tensor(40) 2796
precompute lost 1560 1599
min seq : 2, max seq : 7
loss 0 : 0.09801327198057705
0 loss: 0.09801[0m
gradient norm: 0.39197[0m
Train Epoch: 10 	Loss: 0.09801
The IEKF nets are saved in the file ../temp/iekfnets.p
Amount of time spent for 10 epoch: 5s

N :  1000 N0 :  13510
clip length:  tensor(36) 2796
precompute lost 544 579
min seq : 2, max seq : 8
loss 0 : 0.17307121686501548
0 loss: 0.17307[0m
gradient norm: 1.05062[0m
Train Epoch: 11 	Loss: 0.17307
The IEKF nets are saved in the file ../temp/iekfnets.p
Amount of time spent for 11 epoch: 5s

N :  1000 N0 :  31960
clip length:  tensor(38) 2796
precompute lost 1280 1317
min seq : 2, max seq : 8
loss 0 : 0.245973869576908
0 loss: 0.24597[0m
gradient norm: 0.57057[0m
Train Epoch: 12 	Loss: 0.24597
The IEKF nets are saved in the file ../temp/iekfnets.p
Amount of time spent for 12 epoch: 5s

N :  1000 N0 :  4180
clip length:  tensor(40) 2796
precompute lost 168 207
min seq : 2, max seq : 7
loss 0 : 12.123103165556824
0 loss: 12.12310[0m
[33mgradient norm: 23.04694[0m
Train Epoch: 13 	Loss: 12.12310
The IEKF nets are saved in the file ../temp/iekfnets.p
Amount of time spent for 13 epoch: 5s

N :  1000 N0 :  26150
clip length:  tensor(38) 2796
precompute lost 1048 1085
min seq : 2, max seq : 11
loss 0 : 0.4355499000153068
0 loss: 0.43555[0m
gradient norm: 2.05317[0m
Train Epoch: 14 	Loss: 0.43555
The IEKF nets are saved in the file ../temp/iekfnets.p
Amount of time spent for 14 epoch: 5s

N :  1000 N0 :  54380
clip length:  tensor(40) 2796
precompute lost 2176 2215
min seq : 2, max seq : 7
loss 0 : 0.8199007901636518
0 loss: 0.81990[0m
gradient norm: 1.00760[0m
Train Epoch: 15 	Loss: 0.81990
The IEKF nets are saved in the file ../temp/iekfnets.p
Amount of time spent for 15 epoch: 5s

N :  1000 N0 :  22080
clip length:  tensor(40) 2796
precompute lost 884 923
min seq : 2, max seq : 7
loss 0 : 0.4225525137093125
0 loss: 0.42255[0m
gradient norm: 1.01502[0m
Train Epoch: 16 	Loss: 0.42255
The IEKF nets are saved in the file ../temp/iekfnets.p
Amount of time spent for 16 epoch: 5s

N :  1000 N0 :  13810
clip length:  tensor(36) 2796
precompute lost 556 591
min seq : 2, max seq : 8
loss 0 : 0.21127605481650333
0 loss: 0.21128[0m
gradient norm: 1.67206[0m
Train Epoch: 17 	Loss: 0.21128
The IEKF nets are saved in the file ../temp/iekfnets.p
Amount of time spent for 17 epoch: 5s

N :  1000 N0 :  620
clip length:  tensor(36) 2796
precompute lost 28 63
min seq : 2, max seq : 8
loss 0 : 10.479866505647072
0 loss: 10.47987[0m
[33mgradient norm: 90.43490[0m
Train Epoch: 18 	Loss: 10.47987
The IEKF nets are saved in the file ../temp/iekfnets.p
Amount of time spent for 18 epoch: 5s

N :  1000 N0 :  45330
clip length:  tensor(37) 2796
precompute lost 1816 1852
min seq : 2, max seq : 8
loss 0 : 0.09798515828332556
0 loss: 0.09799[0m
gradient norm: 0.48984[0m
Train Epoch: 19 	Loss: 0.09799
The IEKF nets are saved in the file ../temp/iekfnets.p
Amount of time spent for 19 epoch: 5s

N :  1000 N0 :  0
clip length:  tensor(40) 2796
precompute lost 0 39
min seq : 1, max seq : 8
loss 0 : 46802.95387901155
[33m0 loss is too high 46802.95388[0m
Traceback (most recent call last):
  File "main_usv.py", line 363, in <module>
    launch(args)  # 'args' Í∞ùÏ≤¥Î•º Ï†ÑÎã¨
  File "main_usv.py", line 28, in launch
    train_filter(args, dataset)
  File "/home/wisrl/pjw/ai-imu/src/train_torch_filter.py", line 97, in train_filter
    loss_train, grad_norm, N0 = train_loop(args, dataset, epoch, iekf, optimizer, args.seq_dim)
TypeError: cannot unpack non-iterable NoneType object
